# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file

# Allow all crawlers
User-agent: *
Allow: /

# Sitemap location
Sitemap: https://eurostats.app/sitemap.xml

# Disallow admin/auth pages
Disallow: /users/sign_in
Disallow: /users/sign_up
Disallow: /users/password
Disallow: /users/confirmation
Disallow: /users/unlock

# Allow search engine bots to crawl all public pages
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

# Crawl-delay for less important bots
User-agent: Yandex
Crawl-delay: 10

User-agent: Baiduspider
Crawl-delay: 10
